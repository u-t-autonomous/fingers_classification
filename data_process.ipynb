{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Name: Mohammed\n",
    "import operator\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import itertools\n",
    "from math import ceil\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import decomposition\n",
    "from scipy.stats import zscore\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "def correlate(channel_data1, channel_data2):\n",
    "    x = channel_data1\n",
    "    y = channel_data2\n",
    "    x = MAV_segments(x,10)\n",
    "    y = MAV_segments(y,10)\n",
    "    xc = np.correlate(x, y, mode='full')\n",
    "    xc /= xc[xc.argmax()]\n",
    "    return xc\n",
    "\n",
    "def accuracy(y_pred, y_test):\n",
    "    return 1 - np.linalg.norm(np.array(y_pred) - np.array(y_test),ord = 0)/float(len(y_pred))\n",
    "\n",
    "def relabel(Y):\n",
    "    for i, label_ in enumerate(Y):\n",
    "        if label_ == 0:\n",
    "            if i != 0:\n",
    "                if Y[i-1] == 1:\n",
    "                    Y[i] = 6\n",
    "                if Y[i-1] == 2:\n",
    "                    Y[i] = 7\n",
    "                if Y[i-1] == 3:\n",
    "                    Y[i] = 8\n",
    "                if Y[i-1] == 4:\n",
    "                    Y[i] = 9\n",
    "                if Y[i-1] == 5:\n",
    "                    Y[i] = 10\n",
    "                if Y[i-1] == 0:\n",
    "                    Y[i] = random.choice([6,7,8,9,10])\n",
    "            else:\n",
    "                Y[i] = 10\n",
    "    return Y\n",
    "\n",
    "def load_data(path):\n",
    "    allFiles = glob.glob(path + \"/new_data*.txt\")\n",
    "    frame = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_,index_col=None, header=0)\n",
    "        list_.append(df)\n",
    "    frame = pd.concat(list_,ignore_index = True)\n",
    "    return frame\n",
    "def load_data_(path):\n",
    "    allFiles = [path + \"/new_data_main.txt\"]\n",
    "    frame = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_,index_col=None, header=0)\n",
    "        list_.append(df)\n",
    "    frame = pd.concat(list_,ignore_index = True)\n",
    "    return frame\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def TD(channel_data):\n",
    "    features = [MAV(channel_data)] + MAV_segments(channel_data) + diff_MAV(channel_data) + [ZC(channel_data)] +\\\n",
    "    [SSC(channel_data)] + [WL(channel_data)]\n",
    "    return features\n",
    "\n",
    "def MAV(channel_data):\n",
    "    return sum(map(abs,channel_data))/len(channel_data)\n",
    "\n",
    "def segment_window(channel_data, n_segments = 5.):\n",
    "    seg_length = int(ceil(len(channel_data)/n_segments))\n",
    "    segmented_data = [channel_data[x:x+seg_length] for x in range(0,len(channel_data),seg_length)]\n",
    "    return segmented_data\n",
    "\n",
    "def MAV_segments(channel_data, n_segments = 5.):\n",
    "    segmented_data = segment_window(channel_data, n_segments)\n",
    "    return map(MAV,segmented_data)\n",
    "\n",
    "def diff_MAV(channel_data):\n",
    "    segmented_data = segment_window(channel_data)\n",
    "    prev_segments = segmented_data[:-1]\n",
    "    next_segments = segmented_data[1:]\n",
    "    prev_segments_MAV = map(MAV,prev_segments)\n",
    "    next_segments_MAV = map(MAV,next_segments)\n",
    "    return map(operator.sub, next_segments_MAV, prev_segments_MAV)\n",
    "\n",
    "def ZC(channel_data, threshold = 10):\n",
    "    prev_sample = channel_data[:-1]\n",
    "    next_sample = channel_data[1:]\n",
    "    left_side = map(abs,map(operator.sub, next_sample, prev_sample))\n",
    "    right_side = map(abs,map(operator.add, next_sample, prev_sample))\n",
    "    res = map(operator.sub, left_side, right_side)\n",
    "    res = [1 if (x >= 0 and left > threshold) else 0 for x,left in zip(res,left_side)]\n",
    "    return sum(res)/float(len(prev_sample)+1)\n",
    "\n",
    "def SSC(channel_data, threshold = 5):\n",
    "    the_sample = channel_data[1:-1]\n",
    "    prev_sample = channel_data[:-2]\n",
    "    next_sample = channel_data[2:]\n",
    "    res = [1 if ((x > max(x_prev,x_next) or x < min(x_prev,x_next)) and max(abs(x_next - x),abs(x - x_prev)) > threshold)\\\n",
    "           else 0 for x,x_prev,x_next in zip(the_sample,prev_sample,next_sample)]\n",
    "    return sum(res)/float(len(prev_sample)+2)\n",
    "\n",
    "def WL(channel_data):\n",
    "    prev_sample = channel_data[:-1]\n",
    "    next_sample = channel_data[1:]\n",
    "    diff = map(abs,map(operator.sub, next_sample, prev_sample))\n",
    "    return sum(diff)/float(len(prev_sample)+1)\n",
    "    \n",
    "def SPM_features(data,domain,req_freq,width):\n",
    "    new_mag = []\n",
    "    new_angle = []\n",
    "    mag = np.abs(data)\n",
    "    angle = np.angle(data)\n",
    "    for freq in req_freq:\n",
    "        mag_agg = []\n",
    "        angle_agg = []\n",
    "        for i in range(0,len(mag)):\n",
    "            if domain[i] >= (freq - width/2.) and  domain[i] <= (freq + width/2.):\n",
    "                mag_agg.append(mag[i])\n",
    "                angle_agg.append(angle[i])\n",
    "        new_mag.append(sum(mag_agg)/float(len(mag_agg)))\n",
    "        new_angle.append(sum(angle_agg)/float(len(angle_agg)))\n",
    "    return new_mag+new_angle\n",
    "        \n",
    "def SPM(channel_data, freq = 250):\n",
    "    width = 2.\n",
    "    z = np.fft.rfft(channel_data) # FFT\n",
    "    y = np.fft.rfftfreq(len(channel_data), d = 1./freq) # Frequency data\n",
    "    z = zscore(z)\n",
    "    req_freq = np.arange(5,125,width)\n",
    "    return SPM_features(z,y,req_freq,width)\n",
    "\n",
    "def make_features(channel_data):\n",
    "    return TD(channel_data)+SPM(channel_data)\n",
    "\n",
    "def opt_clf(X, y, params, clf, key, key2=None,key2_arg=None, key3=None,key3_arg=None):\n",
    "    acc_log = []\n",
    "    args = {}\n",
    "    for lambda_ in params:\n",
    "        args[key] = lambda_\n",
    "        if key2 == None:\n",
    "            classifier = clf(**args)\n",
    "        else:\n",
    "            for i, k in enumerate(key2):\n",
    "                args[k] = key2_arg[i]\n",
    "            classifier = clf(**args)\n",
    "        # Split the data into a training set and a test set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random.randint(0,100))\n",
    "        # Run classifier\n",
    "        y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
    "        y_pred_train = classifier.fit(X_train, y_train).predict(X_train)\n",
    "        y_pred = [x if x < 6 else 0 for x in y_pred]\n",
    "        y_test = [x if x < 6 else 0 for x in y_test]\n",
    "        y_pred_train = [x if x < 6 else 0 for x in y_pred_train]\n",
    "        y_train = [x if x < 6 else 0 for x in y_train]\n",
    "        print 'Accuracy for {}: Testing {}, Training {}'.format(lambda_, accuracy(y_test,y_pred), accuracy(y_train,y_pred_train))\n",
    "        acc_log.append(accuracy(y_test,y_pred))\n",
    "        \n",
    "    axes = plt.gca()\n",
    "    axes.scatter(params,acc_log)\n",
    "    axes.set_xlim(min(params)-1,max(params)+1)\n",
    "    # training with the best params\n",
    "    max_index, max_value = max(enumerate(acc_log), key=operator.itemgetter(1))\n",
    "    param = params[max_index]\n",
    "    args[key] = param\n",
    "    classifier = clf(**args)\n",
    "    scores = cross_val_score(classifier, X, y, cv=5,n_jobs = -1)\n",
    "    print \"Cross-validation scores\", scores\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random.randint(0,100))\n",
    "    # Run classifier\n",
    "    y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
    "    y_pred = [x if x < 6 else 0 for x in y_pred]\n",
    "    y_test = [x if x < 6 else 0 for x in y_test]\n",
    "    # Compute confusion matrix\n",
    "    class_names_d = ['None','Thumb','Index','Middle','Ring','Pinky']\n",
    "    class_names = [0, 1, 2, 3, 4, 5]\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred, labels = class_names)\n",
    "    np.set_printoptions(precision=2)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names_d,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names_d, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "    return classifier\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# loading the data\n",
    "path =r'data_250' # use your path\n",
    "frame = load_data_(path)\n",
    "\n",
    "# data pre-processing\n",
    "frame.sort(columns='time', axis=0, ascending=True, inplace=True, kind='quicksort', na_position='last')\n",
    "frame.reset_index(inplace = True)\n",
    "del frame['index']\n",
    "label = frame['label']\n",
    "X = np.array([frame['chan4_thumb'].tolist(), frame['chan5_index'].tolist(), frame['chan6_middle'].tolist(),\\\n",
    " frame['chan7_ring'].tolist(), frame['chan8_pinky'].tolist()])\n",
    "shape = np.shape(X)\n",
    "X = np.array(X).reshape((shape[1],shape[0]))\n",
    "Y = label\n",
    "current_label = 9\n",
    "start_window = -1\n",
    "\n",
    "start_end = []\n",
    "\n",
    "# for i,x in enumerate(frame.label):\n",
    "#     if current_label != x:\n",
    "#         current_label = x\n",
    "#         if start_window == -1:\n",
    "#             start_window = i\n",
    "#         else:\n",
    "#             start_end.append((start_window,i,frame.label[i-1]))\n",
    "#             start_window = i\n",
    "\n",
    "list1 = np.arange(0,frame.shape[0],250)\n",
    "starts = list1[:-1]\n",
    "ends = list1[1:]\n",
    "for i, start in enumerate(starts):\n",
    "    c = Counter(frame.label[start:ends[i]])\n",
    "    start_end.append((start,ends[i],c.most_common()[0][0]))\n",
    "\n",
    "data_points = []\n",
    "\n",
    "for tup in start_end:\n",
    "    data_points.append([frame.chan4_thumb[tup[0]:tup[1]].tolist(),frame.chan5_index[tup[0]:tup[1]].tolist(),\\\n",
    "        frame.chan6_middle[tup[0]:tup[1]].tolist(),frame.chan7_ring[tup[0]:tup[1]].tolist(),frame.chan8_pinky[tup[0]:tup[1]].tolist(),tup[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# feature extraction\n",
    "data = []\n",
    "Y = []\n",
    "for i, data_point in enumerate(data_points):\n",
    "    data.append(map(make_features,data_point[0:5]))\n",
    "    Y.append(data_point[5])\n",
    "    \n",
    "# relabeling data\n",
    "# Y = relabel(Y)\n",
    "# class_names = [1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = np.array(data)\n",
    "shape = np.shape(data)\n",
    "data = np.reshape(data,(shape[0],shape[1]*shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = zscore(data[~np.isnan(data).any(axis=1)])\n",
    "y = np.array(Y)[~np.isnan(data).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "trees = [2,3,4,5,10, 20, 30, 40, 50, 60]\n",
    "layers = [(10,),(20,),(30,),(40,),(50,),(100,)]\n",
    "reg = [.5,1,1.5,2,2.5,3,3.5]\n",
    "# neighbors = [1,2,3,4,5,6]\n",
    "Cs = [.75]\n",
    "depth = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "# pca = PCA(n_components=300)\n",
    "# X_pcaed = pca.fit_transform(X)\n",
    "clf = opt_clf(X, y, depth, RandomForestClassifier, 'max_depth',['n_estimators','n_jobs','class_weight'],[25,-1,{0:.1,1:.4,2:.6,3:1,4:1,5:1}])\n",
    "# opt_clf(X, y, reg, MLPClassifier, 'alpha',['hidden_layer_sizes'],[(1,300)])\n",
    "# clf = opt_clf(X, y, Cs, SVC, 'C',['class_weight','kernel'],['balanced','poly'])\n",
    "plt.show()\n",
    "# # test_clf(X, y, depth, DecisionTreeClassifier, 'max_depth', i)\n",
    "# # test_clf(X, y, Cs, LogisticRegression, 'C', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.estimators_[20].tree_.max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
