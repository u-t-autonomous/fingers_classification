{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Mohammed\n",
    "import operator\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import itertools\n",
    "from math import ceil\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.lda import LDA\n",
    "from sklearn import decomposition\n",
    "from scipy.stats import zscore\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import random\n",
    "\n",
    "\n",
    "def correlate(channel_data1, channel_data2):\n",
    "    x = channel_data1\n",
    "    y = channel_data2\n",
    "    x = MAV_segments(x,10)\n",
    "    y = MAV_segments(y,10)\n",
    "    xc = np.correlate(x, y, mode='full')\n",
    "    xc /= xc[xc.argmax()]\n",
    "    return xc\n",
    "\n",
    "def accuracy(y_pred, y_test):\n",
    "    return 1 - np.linalg.norm(np.array(y_pred) - np.array(y_test),ord = 0)/float(len(y_pred))\n",
    "\n",
    "def relabel(Y):\n",
    "    for i, label_ in enumerate(Y):\n",
    "        if label_ == 0:\n",
    "            if i != 0:\n",
    "                if Y[i-1] == 1:\n",
    "                    Y[i] = 6\n",
    "                if Y[i-1] == 2:\n",
    "                    Y[i] = 7\n",
    "                if Y[i-1] == 3:\n",
    "                    Y[i] = 8\n",
    "                if Y[i-1] == 4:\n",
    "                    Y[i] = 9\n",
    "                if Y[i-1] == 5:\n",
    "                    Y[i] = 10\n",
    "            else:\n",
    "                Y[i] = f\n",
    "    return Y\n",
    "\n",
    "def load_data(path):\n",
    "    allFiles = glob.glob(path + \"/new_data*.txt\")\n",
    "    frame = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_,index_col=None, header=0)\n",
    "        list_.append(df)\n",
    "    frame = pd.concat(list_,ignore_index = True)\n",
    "    return frame\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def TD(channel_data):\n",
    "    features = [MAV(channel_data)] + MAV_segments(channel_data) + diff_MAV(channel_data) + [ZC(channel_data)] +\\\n",
    "    [SSC(channel_data)] + [WL(channel_data)]\n",
    "    return features\n",
    "\n",
    "def MAV(channel_data):\n",
    "    return sum(map(abs,channel_data))/len(channel_data)\n",
    "\n",
    "def segment_window(channel_data, n_segments = 5.):\n",
    "    seg_length = int(ceil(len(channel_data)/n_segments))\n",
    "    segmented_data = [channel_data[x:x+seg_length] for x in range(0,len(channel_data),seg_length)]\n",
    "    return segmented_data\n",
    "\n",
    "def MAV_segments(channel_data, n_segments = 5.):\n",
    "    segmented_data = segment_window(channel_data, n_segments)\n",
    "    return map(MAV,segmented_data)\n",
    "\n",
    "def diff_MAV(channel_data):\n",
    "    segmented_data = segment_window(channel_data)\n",
    "    prev_segments = segmented_data[:-1]\n",
    "    next_segments = segmented_data[1:]\n",
    "    prev_segments_MAV = map(MAV,prev_segments)\n",
    "    next_segments_MAV = map(MAV,next_segments)\n",
    "    return map(operator.sub, next_segments_MAV, prev_segments_MAV)\n",
    "\n",
    "def ZC(channel_data, threshold = 10):\n",
    "    prev_sample = channel_data[:-1]\n",
    "    next_sample = channel_data[1:]\n",
    "    left_side = map(abs,map(operator.sub, next_sample, prev_sample))\n",
    "    right_side = map(abs,map(operator.add, next_sample, prev_sample))\n",
    "    res = map(operator.sub, left_side, right_side)\n",
    "    res = [1 if (x >= 0 and left > threshold) else 0 for x,left in zip(res,left_side)]\n",
    "    return sum(res)/float(len(prev_sample)+1)\n",
    "\n",
    "def SSC(channel_data, threshold = 10):\n",
    "    the_sample = channel_data[1:-1]\n",
    "    prev_sample = channel_data[:-2]\n",
    "    next_sample = channel_data[2:]\n",
    "    res = [1 if ((x > max(x_prev,x_next) or x < min(x_prev,x_next)) and max(abs(x_next - x),abs(x - x_prev)) > threshold)\\\n",
    "           else 0 for x,x_prev,x_next in zip(the_sample,prev_sample,next_sample)]\n",
    "    return sum(res)/float(len(prev_sample)+2)\n",
    "\n",
    "def WL(channel_data):\n",
    "    prev_sample = channel_data[:-1]\n",
    "    next_sample = channel_data[1:]\n",
    "    diff = map(abs,map(operator.sub, next_sample, prev_sample))\n",
    "    return sum(diff)/float(len(prev_sample)+1)\n",
    "    \n",
    "def SPM_features(data,domain,req_freq,width):\n",
    "    new_mag = []\n",
    "    new_angle = []\n",
    "    mag = np.abs(data)\n",
    "    angle = np.angle(data)\n",
    "    for freq in req_freq:\n",
    "        mag_agg = []\n",
    "        angle_agg = []\n",
    "        for i in range(0,len(mag)):\n",
    "            if domain[i] >= (freq - width/2.) and  domain[i] <= (freq + width/2.):\n",
    "                mag_agg.append(mag[i])\n",
    "                angle_agg.append(angle[i])\n",
    "        new_mag.append(sum(mag_agg)/float(len(mag_agg)))\n",
    "        new_angle.append(sum(angle_agg)/float(len(angle_agg)))\n",
    "    return new_mag+new_angle\n",
    "        \n",
    "def SPM(channel_data, freq = 250):\n",
    "    width = 2.\n",
    "    z = np.fft.rfft(channel_data) # FFT\n",
    "    y = np.fft.rfftfreq(len(channel_data), d = 1./freq) # Frequency data\n",
    "    z = zscore(z)\n",
    "    req_freq = np.arange(5,125,width)\n",
    "    return SPM_features(z,y,req_freq,width)\n",
    "\n",
    "def make_features(channel_data):\n",
    "    return TD(channel_data)+SPM(channel_data)\n",
    "\n",
    "def opt_clf(X, y, params, clf, key):\n",
    "    # cross validation\n",
    "    for lambda_ in params:\n",
    "        classifier = clf(**{key:lambda_})\n",
    "        # Split the data into a training set and a test set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random.randint(0,100))\n",
    "        # Run classifier\n",
    "        y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
    "        y_pred = [x if x < 6 else 0 for x in y_pred]\n",
    "        y_test = [x if x < 6 else 0 for x in y_test]\n",
    "        print accuracy(y_test,y_pred)\n",
    "        acc_log.append(accuracy(y_test,y_pred))\n",
    "    axes = plt.gca()\n",
    "    axes.scatter(params,acc_log)\n",
    "    axes.set_xlim(min(params)-1,max(params)+1)\n",
    "    # training with the best params\n",
    "    max_index, max_value = max(enumerate(acc_log), key=operator.itemgetter(1))\n",
    "    param = params[max_index]\n",
    "    classifier = clf(**{key:param})\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random.randint(0,100))\n",
    "    # Run classifier\n",
    "    y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
    "    y_pred = [x if x < 6 else 0 for x in y_pred]\n",
    "    y_test = [x if x < 6 else 0 for x in y_test]\n",
    "    # Compute confusion matrix\n",
    "       \n",
    "    class_names_d = ['None','Thumb','Index','Middle','Ring','Pinky']\n",
    "    class_names = [0, 1, 2, 3, 4, 5]\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred, labels = class_names)\n",
    "    np.set_printoptions(precision=2)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names_d,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names_d, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "    plt.show()\n",
    "\n",
    "# loading the data\n",
    "path =r'data_250' # use your path\n",
    "frame = load_data(path)\n",
    "\n",
    "# data pre-processing\n",
    "# frame = frame[frame['label'] != 0]\n",
    "frame.sort(columns='time', axis=0, ascending=True, inplace=True, kind='quicksort', na_position='last')\n",
    "frame.reset_index(inplace = True)\n",
    "del frame['index']\n",
    "label = frame['label']\n",
    "X = np.array([frame['chan4_thumb'].tolist(), frame['chan5_index'].tolist(), frame['chan6_middle'].tolist(),\\\n",
    " frame['chan7_ring'].tolist(), frame['chan8_pinky'].tolist()])\n",
    "shape = np.shape(X)\n",
    "X = np.array(X).reshape((shape[1],shape[0]))\n",
    "Y = label\n",
    "current_label = 9\n",
    "start_window = -1\n",
    "start_end = []\n",
    "for i,x in enumerate(frame.label):\n",
    "    if current_label != x:\n",
    "        current_label = x\n",
    "        if start_window == -1:\n",
    "            start_window = i\n",
    "        else:\n",
    "            start_end.append((start_window,i,frame.label[i-1]))\n",
    "            # print i\n",
    "            # start_end.append((frame.label[i-1]))\n",
    "            start_window = i\n",
    "data_points = []\n",
    "for tup in start_end:\n",
    "    data_points.append([frame.chan4_thumb[tup[0]:tup[1]].tolist(),frame.chan5_index[tup[0]:tup[1]].tolist(),\\\n",
    "        frame.chan6_middle[tup[0]:tup[1]].tolist(),frame.chan7_ring[tup[0]:tup[1]].tolist(),frame.chan8_pinky[tup[0]:tup[1]].tolist(),tup[2]])\n",
    "\n",
    "# feature extraction\n",
    "data = []\n",
    "Y = []\n",
    "for i, data_point in enumerate(data_points):\n",
    "    data.append(map(make_features,data_point[0:5]))\n",
    "    Y.append(data_point[5])\n",
    "    #(sum(map(abs,data_point[0]))/len(data_point[0]),sum(map(abs,data_point[1]))/len(data_point[1]),sum(map(abs,data_point[2]))/len(data_point[2]),sum(map(abs,data_point[3]))/len(data_point[3]),sum(map(abs,data_point[4]))/len(data_point[4]),data_point[5])\n",
    "\n",
    "# relabeling data\n",
    "Y = relabel(Y)\n",
    "class_names = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "data = np.array(data)\n",
    "shape = np.shape(data)\n",
    "data = np.reshape(data,(shape[0],shape[1]*shape[2]))\n",
    "# # Dimensionality reduction\n",
    "# pca = decomposition.PCA(n_components=35)\n",
    "# pca.fit(data)\n",
    "# data = pca.transform(data)\n",
    "\n",
    "# # ML step\n",
    "# classifier = RandomForestClassifier(n_estimators=200) # best is 200\n",
    "# classifier = KNeighborsClassifier(n_neighbors=1, algorithm='ball_tree') # best is 1\n",
    "# classifier = LogisticRegression(C=0.25) # best is 0.25\n",
    "# classifier = DecisionTreeClassifier(max_depth = 3) # best is 4\n",
    "# scores = cross_val_score(classifier, np.array(data), Y, cv=5)\n",
    "# print scores\n",
    "\n",
    "# X = data\n",
    "# y = Y\n",
    "\n",
    "# acc_log = []\n",
    "# trees = [2400,2400,2400,2400,2400,2400]\n",
    "# neighbors = [1,2,3,4,5,6]\n",
    "# Cs = [.25,.5,.75,1,1.5,2,2.5,3,3.5,4.]\n",
    "# depth = [1,2,3,4,5,6,7,8,9,10]\n",
    "# opt_clf(X, y, trees, RandomForestClassifier, 'n_estimators')\n",
    "# # test_clf(X, y, depth, DecisionTreeClassifier, 'max_depth', i)\n",
    "# # test_clf(X, y, Cs, LogisticRegression, 'C', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_log = []\n",
    "X = zscore(data[~np.isnan(data).any(axis=1)])\n",
    "y = np.array(Y)[~np.isnan(data).any(axis=1)]\n",
    "trees = [2300,2300,2300,2300,2300]\n",
    "# neighbors = [1,2,3,4,5,6]\n",
    "# Cs = [.25,.5,.75,1,1.5,2,2.5,3,3.5,4.]\n",
    "# depth = [1,2,3,4,5,6,7,8,9,10]\n",
    "opt_clf(X, y, trees, RandomForestClassifier, 'n_estimators')\n",
    "# # test_clf(X, y, depth, DecisionTreeClassifier, 'max_depth', i)\n",
    "# # test_clf(X, y, Cs, LogisticRegression, 'C', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([0.575342465753,0.712328767123,\n",
    "0.780821917808,\n",
    "0.780821917808,\n",
    "0.72602739726,\n",
    "0.739726027397])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TS plot\n",
    "# axes1 = plt.subplot(321)\n",
    "# axes1.set_ylim([-600,600])\n",
    "# axes1.plot(ch4[0:100000])\n",
    "# axes1.set_title('Ch4: Thumb')\n",
    "\n",
    "# axes2 = plt.subplot(322)\n",
    "# axes2.plot(ch5[0:100000])\n",
    "# axes2.set_ylim([-400,400])\n",
    "# axes2.set_title('Ch5: Index')\n",
    "\n",
    "# axes3 = plt.subplot(323)\n",
    "# axes3.plot(ch6[0:100000])\n",
    "# axes3.set_ylim([-400,400])\n",
    "# axes3.set_title('Ch6: Middle')\n",
    "\n",
    "# axes4 = plt.subplot(324)\n",
    "# axes4.plot(ch7[0:100000])\n",
    "# axes4.set_ylim([-400,400])\n",
    "# axes4.set_title('Ch7: Ring')\n",
    "\n",
    "# axes5 = plt.subplot(325)\n",
    "# axes5.plot(ch8[0:100000])\n",
    "# axes5.set_ylim([-400,400])\n",
    "# axes5.set_title('Ch8: Pinky')\n",
    "\n",
    "# axes6 = plt.subplot(326)\n",
    "# axes6.plot(label[0:100000]*80)\n",
    "# axes6.set_ylim([-400,400])\n",
    "# axes6.set_title('Label')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# 3D plot\n",
    "# ch4_1 = frame[(frame.label == 3)]['chan4_thumb']\n",
    "# ch5_1 = frame[(frame.label == 3)]['chan5_index']\n",
    "# ch6_1 = frame[(frame.label == 3)]['chan6_middle']\n",
    "# ch4_2 = frame[(frame.label == 2)]['chan4_thumb']\n",
    "# ch5_2 = frame[(frame.label == 2)]['chan5_index']\n",
    "# ch6_2 = frame[(frame.label == 2)]['chan6_middle']\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.scatter(ch4_1, ch5_1, ch6_1, c='r', marker='o')\n",
    "# ax.scatter(ch4_2, ch5_2, ch6_2, c='b', marker='^')\n",
    "# ax.set_xlabel('X Label')\n",
    "# ax.set_ylabel('Y Label')\n",
    "# ax.set_zlabel('Z Label')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# a = fft(data_points[5][0]) # FFT\n",
    "# b = fft(data_points[5][1]) # FFT\n",
    "# c = fft(data_points[5][2]) # FFT\n",
    "# d = fft(data_points[5][3]) # FFT\n",
    "# e = fft(data_points[5][4]) # FFT\n",
    "# y = np.arange(5,65,2.)\n",
    "\n",
    "# # plt.plot(b,z)\n",
    "# # plt.plot(b,z)\n",
    "# # plt.plot(b,z)\n",
    "# # # TS plot\n",
    "# axes1 = plt.subplot(111)\n",
    "# axes1.plot(y,a)\n",
    "# axes1.plot(y,b)\n",
    "# axes1.plot(y,c)\n",
    "# axes1.plot(y,d)\n",
    "# axes1.plot(y,e)\n",
    "# axes1.set_title('Ch4: Thumb')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "x = np.random.randn(n)\n",
    "x = MAV_segments(x,10)\n",
    "xc = np.correlate(x, x, mode='full')\n",
    "xc /= xc[xc.argmax()]\n",
    "xchalf = xc[xc.size / 2:]\n",
    "xchalf_max = xchalf.max()\n",
    "print xchalf_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xc.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
