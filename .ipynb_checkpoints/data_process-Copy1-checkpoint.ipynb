{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 716 ms, sys: 612 ms, total: 1.33 s\n",
      "Wall time: 3.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Name: Mohammed\n",
    "import operator\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import itertools\n",
    "from math import ceil\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import decomposition\n",
    "from scipy.stats import zscore\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "import random\n",
    "from collections import Counter\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.signal import freqz\n",
    "from sklearn.preprocessing import robust_scale\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut=4, highcut=30, fs=250, order=6):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "def correlate(channel_data1, channel_data2):\n",
    "    x = channel_data1\n",
    "    y = channel_data2X = X[y!=0]\n",
    "# y = y[y!=0]\n",
    "    x = MAV_segments(x,10)\n",
    "    y = MAV_segments(y,10)\n",
    "    xc = np.correlate(x, y, mode='full')\n",
    "    xc /= xc[xc.argmax()]\n",
    "    return xc\n",
    "\n",
    "def accuracy(y_pred, y_test):\n",
    "    return 1 - np.linalg.norm(np.array(y_pred) - np.array(y_test),ord = 0)/float(len(y_pred))\n",
    "\n",
    "def relabel(Y):\n",
    "    for i, label_ in enumerate(Y):\n",
    "        if label_ == 0:\n",
    "            Y[i] = 0\n",
    "        elif label_ in [1,2,3]:\n",
    "            Y[i] = 1\n",
    "    return Y\n",
    "\n",
    "\n",
    "def relabel_(Y):\n",
    "    for i, label_ in enumerate(Y):\n",
    "        if label_ == 0:\n",
    "            if i != 0:\n",
    "                if Y[i-1] == 1:\n",
    "                    Y[i] = 1\n",
    "                if Y[i-1] == 2:\n",
    "                    Y[i] = 2\n",
    "                if Y[i-1] == 3:\n",
    "                    Y[i] = 3\n",
    "                if Y[i-1] == 10:\n",
    "                    Y[i] = random.choice([10])\n",
    "            else:\n",
    "                Y[i] = 10\n",
    "    return Y\n",
    "\n",
    "def switch_label(Y,a,b):\n",
    "    for i, label_ in enumerate(Y):\n",
    "        if label_ == a:\n",
    "            Y[i] = b\n",
    "        elif label_ == b:\n",
    "            Y[i] = a\n",
    "    return Y\n",
    "            \n",
    "def relabel__(Y):\n",
    "    Y = switch_label(Y,3,4)\n",
    "    for i, label_ in enumerate(Y):\n",
    "        if label_ == 0:\n",
    "            if i != 0:\n",
    "                if Y[i-1] == 3:\n",
    "                    Y[i] = 10\n",
    "            else:\n",
    "                Y[i] = 10\n",
    "    return Y\n",
    "\n",
    "def load_data(path):\n",
    "    allFiles = glob.glob(path + \"/new_data_main*.txt\")\n",
    "    frame = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_,index_col=None, header=0)\n",
    "        list_.append(df)\n",
    "    frame = pd.concat(list_,ignore_index = True)\n",
    "    return frame\n",
    "\n",
    "def load_data(path):\n",
    "    allFiles = glob.glob(path + \"/new_data_main*.txt\")\n",
    "    frame = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_,index_col=None, header=0)\n",
    "        list_.append(df)\n",
    "    frame = pd.concat(list_,ignore_index = True)\n",
    "    return frame\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i,\"{0:.2f}\".format(cm[i, j]) ,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def TD(channel_data):\n",
    "    features = [MAV(channel_data)] + MAV_segments(channel_data) + diff_MAV(channel_data) + [ZC(channel_data)] +\\\n",
    "    [SSC(channel_data)] + [WL(channel_data)]\n",
    "    return features\n",
    "\n",
    "def MAV(channel_data):\n",
    "    return sum(map(abs,channel_data))/len(channel_data)\n",
    "\n",
    "def segment_window(channel_data, n_segments = 5.):\n",
    "    seg_length = int(ceil(len(channel_data)/n_segments))\n",
    "    segmented_data = [channel_data[x:x+seg_length] for x in range(0,len(channel_data),seg_length)]\n",
    "    return segmented_data\n",
    "\n",
    "def MAV_segments(channel_data, n_segments = 5.):\n",
    "    segmented_data = segment_window(channel_data, n_segments)\n",
    "    return map(MAV,segmented_data)\n",
    "\n",
    "def diff_MAV(channel_data):\n",
    "    segmented_data = segment_window(channel_data)\n",
    "    prev_segments = segmented_data[:-1]\n",
    "    next_segments = segmented_data[1:]\n",
    "    prev_segments_MAV = map(MAV,prev_segments)\n",
    "    next_segments_MAV = map(MAV,next_segments)\n",
    "    return map(operator.sub, next_segments_MAV, prev_segments_MAV)\n",
    "\n",
    "def ZC(channel_data, threshold = 10):\n",
    "    prev_sample = channel_data[:-1]\n",
    "    next_sample = channel_data[1:]\n",
    "    left_side = map(abs,map(operator.sub, next_sample, prev_sample))\n",
    "    right_side = map(abs,map(operator.add, next_sample, prev_sample))\n",
    "    res = map(operator.sub, left_side, right_side)\n",
    "    res = [1 if (x >= 0 and left > threshold) else 0 for x,left in zip(res,left_side)]\n",
    "    return sum(res)/float(len(prev_sample)+1)\n",
    "\n",
    "def SSC(channel_data, threshold = 5):\n",
    "    the_sample = channel_data[1:-1]\n",
    "    prev_sample = channel_data[:-2]\n",
    "    next_sample = channel_data[2:]\n",
    "    res = [1 if ((x > max(x_prev,x_next) or x < min(x_prev,x_next)) and max(abs(x_next - x),abs(x - x_prev)) > threshold)\\\n",
    "           else 0 for x,x_prev,x_next in zip(the_sample,prev_sample,next_sample)]\n",
    "    return sum(res)/float(len(prev_sample)+2)\n",
    "\n",
    "def WL(channel_data):\n",
    "    prev_sample = channel_data[:-1]\n",
    "    next_sample = channel_data[1:]\n",
    "    diff = map(abs,map(operator.sub, next_sample, prev_sample))\n",
    "    return sum(diff)/float(len(prev_sample)+1)\n",
    "    \n",
    "def SPM_features(data,domain,req_freq,width):\n",
    "    new_mag = []\n",
    "    new_angle = []\n",
    "    mag = np.abs(data)\n",
    "    angle = np.angle(data)\n",
    "    for freq in req_freq:\n",
    "        mag_agg = []\n",
    "        angle_agg = []\n",
    "        for i in range(0,len(mag)):\n",
    "            if domain[i] >= (freq - width/2.) and  domain[i] <= (freq + width/2.):\n",
    "                mag_agg.append(mag[i])\n",
    "                angle_agg.append(angle[i])\n",
    "        new_mag.append(sum(mag_agg)/float(len(mag_agg)))\n",
    "        new_angle.append(sum(angle_agg)/float(len(angle_agg)))\n",
    "    return new_mag#+new_angle\n",
    "        \n",
    "def SPM(channel_data, freq = 250):\n",
    "    width = 0.5\n",
    "    z = np.fft.rfft(channel_data) # FFT\n",
    "    y = np.fft.rfftfreq(len(channel_data), d = 1./freq) # Frequency data\n",
    "    #z = zscore(z)\n",
    "    req_freq = [10, 12, 15]#np.arange(2,25,width)\n",
    "    return SPM_features(z,y,req_freq,width)\n",
    "\n",
    "def make_features(channel_data):\n",
    "    return SPM(channel_data)\n",
    "\n",
    "def opt_clf(X, y, params, clf, key, key2=None,key2_arg=None, key3=None,key3_arg=None):\n",
    "    acc_log = []\n",
    "    args = {}\n",
    "    for lambda_ in params:\n",
    "        args[key] = lambda_\n",
    "        if key2 == None:\n",
    "            classifier = clf(**args)\n",
    "        else:\n",
    "            for i, k in enumerate(key2):\n",
    "                args[k] = key2_arg[i]\n",
    "            classifier = clf(**args)\n",
    "        # Split the data into a training set and a test set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random.randint(0,100))\n",
    "        # Run classifier\n",
    "        y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
    "        y_pred_train = classifier.fit(X_train, y_train).predict(X_train)\n",
    "        print 'Accuracy for {}: Testing {}, Training {}'.format(lambda_, accuracy(y_test,y_pred), accuracy(y_train,y_pred_train))\n",
    "        acc_log.append(accuracy(y_test,y_pred))\n",
    "        \n",
    "    axes = plt.gca()\n",
    "    axes.scatter(params,acc_log)\n",
    "    axes.set_xlim(min(params)-1,max(params)+1)\n",
    "    # training with the best params\n",
    "    max_index, max_value = max(enumerate(acc_log), key=operator.itemgetter(1))\n",
    "    param = params[max_index]\n",
    "    args[key] = param\n",
    "    classifier = clf(**args)\n",
    "    scores = cross_val_score(classifier, X, y, cv=5,n_jobs = -1)\n",
    "    print \"Cross-validation scores\", scores\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random.randint(0,100))\n",
    "    # Run classifier\n",
    "    y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
    "    y_pred = [x if x <= 4 else 0 for x in y_pred]\n",
    "    y_test = [x if x <= 4 else 0 for x in y_test]\n",
    "    # Compute confusion matrix\n",
    "    class_names_d = ['10Hz', '12Hz', '15Hz']\n",
    "    class_names = [2, 3, 4]\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred, labels = class_names)\n",
    "    np.set_printoptions(precision=2)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names_d,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names_d, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:9: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.2 s, sys: 588 ms, total: 13.7 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fs = 250\n",
    "lowcut = 2\n",
    "highcut = 35\n",
    "\n",
    "# loading the data\n",
    "path =r'data_ssvep' # use your path\n",
    "frame = load_data(path)\n",
    "\n",
    "# data pre-processing\n",
    "frame.sort(columns='time', axis=0, ascending=True, inplace=True, kind='quicksort', na_position='last')\n",
    "frame.reset_index(inplace = True)\n",
    "del frame['index']\n",
    "label = frame['label']\n",
    "start_end = []\n",
    "list1 = np.arange(0,frame.shape[0],50)\n",
    "starts = list1[:-10]\n",
    "ends = list1[10:]\n",
    "for i, start in enumerate(starts):\n",
    "    c = Counter(frame.label[start:ends[i]])\n",
    "    if len(set(list(c.elements()))) == 1:\n",
    "        label = c.most_common()[0][0]\n",
    "    else:\n",
    "        label = 10\n",
    "    start_end.append((start,ends[i],label,frame.time[ends[i]]))\n",
    "data_points = []\n",
    "\n",
    "# for tup in start_end:\n",
    "#     data_points.append([frame.chan1[tup[0]:tup[1]].tolist(),frame.chan2[tup[0]:tup[1]].tolist(),\\\n",
    "#                         frame.chan3[tup[0]:tup[1]].tolist(),frame.chan4[tup[0]:tup[1]].tolist(),\\\n",
    "#                         frame.chan5[tup[0]:tup[1]].tolist(),frame.chan6[tup[0]:tup[1]].tolist(),\\\n",
    "#                         frame.chan7[tup[0]:tup[1]].tolist(),frame.chan8[tup[0]:tup[1]].tolist(),tup[2]])\n",
    "\n",
    "# tup = (0,200,1)\n",
    "# data_points.append([frame.chan1[tup[0]:tup[1]].tolist(),frame.chan2[tup[0]:tup[1]].tolist(),\\\n",
    "#                     frame.chan3[tup[0]:tup[1]].tolist(),frame.chan4[tup[0]:tup[1]].tolist(),\\\n",
    "#                     frame.chan5[tup[0]:tup[1]].tolist(),frame.chan6[tup[0]:tup[1]].tolist(),\\\n",
    "#                     frame.chan7[tup[0]:tup[1]].tolist(),frame.chan8[tup[0]:tup[1]].tolist(),tup[2]])\n",
    "\n",
    "for tup in start_end:\n",
    "    data_points.append([frame.chan7[tup[0]:tup[1]].tolist(),frame.chan8[tup[0]:tup[1]].tolist(), tup[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.2 s, sys: 8 ms, total: 13.2 s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# feature extraction\n",
    "data_ = []\n",
    "Y_ = []\n",
    "for i, data_point in enumerate(data_points):\n",
    "    data_.append(map(make_features,data_point[0:2]))\n",
    "    Y_.append(data_point[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 ms, sys: 0 ns, total: 28 ms\n",
      "Wall time: 28.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_ = np.array(data_)\n",
    "shape = np.shape(data_)\n",
    "# data = data_\n",
    "data = np.reshape(data_,(shape[0],shape[1]*shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 12 ms, total: 44 ms\n",
      "Wall time: 30.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Y__ = Y_[:]\n",
    "# Y = relabel(Y__)\n",
    "Y = relabel__(Y__)\n",
    "# Y = Y__[:]\n",
    "# X = robust_scale(data[~np.isnan(data).any(axis=1)],axis=1)\n",
    "X = data[~np.isnan(data).any(axis=1)]\n",
    "y = np.array(Y)[~np.isnan(data).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 40: Testing 0.740038560411, Training 0.75589117395\n",
      "Accuracy for 45: Testing 0.750321336761, Training 0.753427592117\n",
      "Accuracy for 50: Testing 0.750964010283, Training 0.753106255356\n",
      "Accuracy for 55: Testing 0.753213367609, Training 0.752570694087\n",
      "Accuracy for 60: Testing 0.752249357326, Training 0.752035132819\n",
      "Cross-validation scores [ 0.80979133  0.87263961  0.69746886  0.63881077  0.64443552]\n",
      "Confusion matrix, without normalization\n",
      "[[756 118 141]\n",
      " [147 798 115]\n",
      " [130 103 804]]\n",
      "Normalized confusion matrix\n",
      "[[ 0.74  0.12  0.14]\n",
      " [ 0.14  0.75  0.11]\n",
      " [ 0.13  0.1   0.78]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/pymodules/python2.7/matplotlib/collections.py:548: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == 'face':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.04 s, sys: 5.52 s, total: 9.56 s\n",
      "Wall time: 9.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trees = [100,500,1000,2000]\n",
    "layers = [(10,),(20,),(30,),(40,),(50,),(100,)]\n",
    "reg = [1]\n",
    "neighbors = [1,2,3,4,5,6]\n",
    "Cs = [40,45,50,55,60]\n",
    "depth = [15]\n",
    "max_f = [20,20,20]\n",
    "# pca = PCA(n_components=50)\n",
    "# X_pcaed = pca.fit_transform(X)\n",
    "# X_tmp = X[y!=0]\n",
    "# y_tmp = y[y!=0]\n",
    "# X_new = X_tmp[y_tmp!=10]\n",
    "# y_new = y_tmp[y_tmp!=10]\n",
    "# X_new = X[y!=10]\n",
    "# y_new = y[y!=10]\n",
    "mask = (y != 10) & (y != 0) & (y != 1)\n",
    "X_new = X[mask]\n",
    "y_new = y[mask]\n",
    "clf = opt_clf(X_new[:], y_new[:], Cs, LogisticRegression, 'C',['penalty','class_weight'],['l1','balanced'])\n",
    "# clf = opt_clf(X_new[:], y_new[:], depth, RandomForestClassifier, 'max_depth',['n_jobs','class_weight','n_estimators','max_features'],[-1,'balanced',20,6])\n",
    "# clf = opt_clf(X_new, y_new, reg, MLPClassifier, 'alpha',['hidden_layer_sizes', 'alpha'],[(500,6), 1])\n",
    "# clf = opt_clf(X_new, y_new, Cs, SVC, 'C',['class_weight','kernel'],['balanced','rbf'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle\n",
    "# save the classifier\n",
    "with open('LR_classifier.pkl', 'wb') as fid:\n",
    "    cPickle.dump(clf, fid)    \n",
    "\n",
    "# load it again\n",
    "with open('LR_classifier.pkl', 'rb') as fid:\n",
    "    clf_loaded = cPickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft\n",
    "# Number of sample points\n",
    "N = 1356\n",
    "# sample spacing\n",
    "T = 1.0 / 250.0\n",
    "x = np.linspace(0.0, N*T, N)\n",
    "y = frame[start_end[1][0]:start_end[1][1]].chan1\n",
    "yf = fft(y)\n",
    "xf = np.linspace(0.0, 1.0/(16*T), N/16)\n",
    "plt.plot(xf, 2.0/N * np.abs(yf[0:N/2])[0:N/16])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "def reject_outliers(data, m=2):\n",
    "    return data[abs(data - np.mean(data)) < m * np.std(data)]\n",
    "X_new = map(reject_outliers,X.reshape(X.shape[1],X.shape[0]))\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
